{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttHtSg1NesDC",
        "outputId": "ce023a51-752a-47e1-ebd7-0e568940d726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing (preprocess.py)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", ' ', text)\n",
        "    tokens = [t for t in text.split() if t not in stop]\n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8eM7YOncetNP"
      },
      "outputs": [],
      "source": [
        "# Indexing (indexer.py)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define 'docs' as a list of strings (example placeholder)\n",
        "docs = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "docs_processed = [preprocess_text(d) for d in docs]\n",
        "X = vectorizer.fit_transform(docs_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB6oYa2KfLwg",
        "outputId": "2042266f-1873-4c07-9be3-59fd11e411ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# BM25 (retrieve.py)\n",
        "!pip install rank_bm25\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "\n",
        "tokenized_docs = [doc.split() for doc in docs_processed]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "def bm25_query(q):\n",
        "    q_tok = preprocess_text(q).split()\n",
        "    scores = bm25.get_scores(q_tok)\n",
        "    ranked = np.argsort(scores)[::-1]\n",
        "    return ranked, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RP25XxzWfXyl"
      },
      "outputs": [],
      "source": [
        "# Retrieval (retrieve.py)\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "def tfidf_query(q):\n",
        "    q_vec = vectorizer.transform([preprocess_text(q)])\n",
        "    sims = linear_kernel(q_vec, X).flatten()\n",
        "    ranked = np.argsort(sims)[::-1]\n",
        "    return ranked, sims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wqMFYrzeVpKd"
      },
      "outputs": [],
      "source": [
        "class CISIParser:\n",
        "    @staticmethod\n",
        "    def _parse_cisi_content(file_path, target_tags):\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        parsed_data = {}\n",
        "        items = content.split('.I ')\n",
        "\n",
        "        for item in items[1:]:  # Skip empty preamble\n",
        "            lines = item.split('\\n')\n",
        "            try:\n",
        "                obj_id = int(lines[0].strip())\n",
        "            except ValueError:\n",
        "                continue  # Skip malformed IDs\n",
        "\n",
        "            collected_text = []\n",
        "            current_tag = None\n",
        "\n",
        "            for line in lines:\n",
        "                if line.startswith('.'):\n",
        "                    # Update current state (e.g., .T, .W, .A)\n",
        "                    current_tag = line[:2]\n",
        "                    continue\n",
        "\n",
        "                # If we are currently inside one of the tags we want, keep the line\n",
        "                if current_tag in target_tags:\n",
        "                    collected_text.append(line)\n",
        "\n",
        "            parsed_data[obj_id] = \" \".join(collected_text).strip()\n",
        "\n",
        "        return parsed_data\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_docs(file_path):\n",
        "        # Documents need Title (.T) and Abstract (.W)\n",
        "        return CISIParser._parse_cisi_content(file_path, target_tags=['.T', '.W'])\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_titles(file_path):\n",
        "        # Reporting only needs Title (.T)\n",
        "        return CISIParser._parse_cisi_content(file_path, target_tags=['.T'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Th1VWugVppK"
      },
      "outputs": [],
      "source": [
        "@staticmethod\n",
        "def parse_queries(file_path):\n",
        "    # Queries usually only have body text (.W)\n",
        "    # Note: Some CISI queries have .T too, but usually .W is the core question\n",
        "    return CISIParser._parse_cisi_content(file_path, target_tags=['.W'])\n",
        "\n",
        "@staticmethod\n",
        "def parse_rels(file_path):\n",
        "    # This format is completely different (Structure: QID DOCID ...),\n",
        "    # so it stays as its own distinct logic.\n",
        "    rels = defaultdict(set)\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                try:\n",
        "                    qid = int(parts[0])\n",
        "                    doc_id = int(parts[1])\n",
        "                    rels[qid].add(doc_id)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return rels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fWfRCJ8vVp4h"
      },
      "outputs": [],
      "source": [
        "class VectorSpaceModel:\n",
        "    def __init__(self, docs):\n",
        "        self.doc_ids = list(docs.keys())\n",
        "        self.corpus = [docs[did] for did in self.doc_ids]\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            tokenizer=tokenize,\n",
        "            stop_words=None,\n",
        "            token_pattern=None\n",
        "        )\n",
        "        self.doc_vectors = self.vectorizer.fit_transform(self.corpus)\n",
        "\n",
        "    def retrieve(self, query_text):\n",
        "        q_vec = self.vectorizer.transform([query_text])\n",
        "        scores = cosine_similarity(q_vec, self.doc_vectors).flatten()\n",
        "        ranked_indices = scores.argsort()[::-1]\n",
        "        results = []\n",
        "        for idx in ranked_indices:\n",
        "            if scores[idx] > 0:\n",
        "                results.append((self.doc_ids[idx], scores[idx]))\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nriZoDYmbAc2"
      },
      "outputs": [],
      "source": [
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "  print(\"Parsing CISI Dataset...\")\n",
        "  # Loading everything up\n",
        "  docs = CISIParser.parse_docs(FILES['docs'])\n",
        "\n",
        "  titles = CISIParser.parse_titles(FILES['docs'])\n",
        "  queries = CISIParser.parse_queries(FILES['queries'])\n",
        "  rels = CISIParser.parse_rels(FILES['rels'])\n",
        "\n",
        "  print(f\"Loaded {len(docs)} documents, {lenn(queries)} queries, {len(rels)} relevance sets.\")\n",
        "\n",
        "  print(\"\\nInitializing Models...\")\n",
        "  # initializing out two contenders\n",
        "  vsm = VectorSpaceModel(docs)\n",
        "  lm = DirichletLM(docs, mu=4000)\n",
        "\n",
        "  print(\"\\nRunning Retrieval...\")\n",
        "  # running the retrieval loop\n",
        "  vsm_map_scores, vsm_p10_scores = [], []\n",
        "  lm_map_scores, lm_p10_scores = [], []\n",
        "\n",
        "  active_queries = sorted([qid for qid in queries in rels])\n",
        "  results_header = f\"{'Model': <10} | {'Query ID':<10} | {'Doc ID':<10} | {'Title' :<60} | {'Query':<50}\\n\"\n",
        "  seperator = \"-\" * 150 + \"\\n\"\n",
        "\n",
        "  with open(RESULT_FILE, 'w', encoding='utf-8') as f, open(SAMPLE_RESULT_FILE, 'w', encoding='utf-8') as sample_f:\n",
        "    f.write(result_header)\n",
        "    f.write(seperator)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
